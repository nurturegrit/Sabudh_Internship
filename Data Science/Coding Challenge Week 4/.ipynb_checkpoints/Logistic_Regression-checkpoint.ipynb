{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea002b1-9356-49ef-96e5-89b272ffe9db",
   "metadata": {},
   "source": [
    "## Machine Learning Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c062e4-0b5b-4650-bef0-ae1061541153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845923a-4717-4a84-818f-1853806562eb",
   "metadata": {},
   "source": [
    "# Function to Generate an (m+1)-Dimensional Data Set\n",
    "\n",
    "This function generates a dataset consisting of `m` continuous independent variables (X) and a binary dependent variable (Y). The relationship between X and Y is determined using a logistic function. To introduce noise to the labels (Y), we assume a Bernoulli distribution with a probability of flipping the label, determined by a parameter `θ`. The higher the value of `θ`, the greater the noise in the dataset.\n",
    "\n",
    "## Parameters:\n",
    "\n",
    "- `θ`: The probability of flipping the label (Y).\n",
    "- `n`: The number of samples (size of the dataset).\n",
    "- `m`: The number of independent variables.\n",
    "\n",
    "## Outputs:\n",
    "\n",
    "- `X`: An `n x (m+1)` numpy array of independent variable values, with the first column set to 1.\n",
    "- `Y`: An `n x 1` numpy array of binary output values (0 or 1).\n",
    "- `β`: A random vector of size `(m+1, 1)` representing the coefficients used to generate Y from X.\n",
    "\n",
    "## Mathematical Definition:\n",
    "\n",
    "The binary dependent variable `Y` is generated as follows:\n",
    "\n",
    "$$\n",
    "Y_i = \n",
    "\\begin{cases} \n",
    "1 & \\text{if} \\ \\frac{1}{1 + \\exp(-\\mathbf{x}_i \\cdot \\mathbf{\\beta})} > 0.5 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `X` is the matrix of independent variables.\n",
    "- `β` is the coefficient vector.\n",
    "- `θ` is the probability of flipping the label.\n",
    "\n",
    "The labels are flipped with a probability `θ` to introduce noise into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08a3b72-5225-4f11-a62e-994ac0ff13dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n, m, theta):\n",
    "    #Generate X\n",
    "    X = np.random.randn(n, m)\n",
    "    X = np.hstack((np.ones((n, 1)), X))\n",
    "\n",
    "    # Generate Coefficient Vector\n",
    "    B = np.random.random(size=(m+1, 1))\n",
    "\n",
    "    # Generate Y vector according to the given formula\n",
    "    z = - np.dot(X, B)\n",
    "    var = np.exp(z)\n",
    "    intermediate_y = 1/(1 + var)\n",
    "    Y = np.where(intermediate_y > 0.5, 1, 0)\n",
    "\n",
    "    # Add Noise to Y\n",
    "    # np.random.random makes a 1D n vector where values range 0 to 1 with equal prob\n",
    "    # vector < theta represents 1 for all values when value is less than theta\n",
    "    # effectively 1 occurs with theta probability in vector noise\n",
    "    noise = np.random.random(n) < theta\n",
    "    Y = np.abs(Y - np.reshape(noise, Y.shape))\n",
    "    return X, Y, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d335482-3c59-4fa4-9ec0-74c3fa1b6e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 6), (100, 1), (6, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "theta = 0.1  # 10% probability of flipping labels\n",
    "n = 100  # 100 samples\n",
    "m = 5    # 5 independent variables\n",
    "\n",
    "X, Y, B = generate_dataset(n, m, theta)\n",
    "\n",
    "# Output the generated dataset and coefficients\n",
    "X.shape, Y.shape, B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe33a8-a937-46e7-99fb-0b36f566d886",
   "metadata": {},
   "source": [
    "Write a function that learns the parameters of a logistic regression function given inputs:\n",
    "- **X**: An n × m numpy array of independent variable values\n",
    "- **Y**: The n × 1 binary numpy array of output values\n",
    "- **k**: the number of iterations (epochs)\n",
    "- **τ**: the threshold on change in Cost function value from the previous to current iteration\n",
    "- **λ**: the learning rate for Gradient Descent\n",
    "\n",
    "The function should implement the Gradient Descent algorithm as discussed in class that initializes **β** with random values and then updates these values in each iteration by moving in the direction defined by the partial derivative of the cost function with respect to each of the coefficients. The function should use only one loop that ends after a number of iterations (**k**) or a threshold on the change in cost function value (**τ**).\n",
    "\n",
    "The output should be an **m + 1** dimensional vector of coefficients and the final cost function value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a841c9cd-1301-4170-9d75-9545e0ee099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, Y, k, tau, lambda_):\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "        \n",
    "    def cost_function(X, Y, B):\n",
    "        n = len(Y)\n",
    "        intermediate = np.dot(X, B)\n",
    "        predictions = sigmoid(intermediate)\n",
    "        term1 = np.dot(Y.T, np.log(predictions)) # Dot product where first element of term is always transposed\n",
    "        term2 = np.dot((1 - Y).T, np.log(1 - predictions))\n",
    "        cost = -1/n * (term1 + term2)\n",
    "        return cost\n",
    "        \n",
    "    n = X.shape[0]\n",
    "    B = np.random.random(size=(X.shape[1], 1))\n",
    "    Y = Y.reshape(-1, 1)  # Ensure Y is a column vector\n",
    "    cost = cost_function(X, Y, B)\n",
    "    \n",
    "    for _ in range(k):\n",
    "        prev_cost = cost\n",
    "        \n",
    "        # Gradient Descent\n",
    "        predictions = sigmoid(np.dot(X, B))\n",
    "        gradient = np.dot(X.T, (predictions - Y)) / n\n",
    "        B = B - lambda_ * gradient\n",
    "\n",
    "        cost = cost_function(X, Y, B)\n",
    "\n",
    "        # Check threshold\n",
    "        if abs(cost - prev_cost) < tau:\n",
    "            break\n",
    "\n",
    "    return B.flatten(), cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28438d87-cbdf-4344-9bb1-044990cd90a8",
   "metadata": {},
   "source": [
    "Create a report investigating how differen values of n and θ impact the ability for your logistic regression\n",
    "function to learn the coefficients, β, used to generate the output vector Y . Also include your derivation of\n",
    "the partial derivative of the cost function with respect to the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4b1e1df-ba44-45d9-b1df-b1adf774dce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 100, θ: 0.1, Accuracy: 0.77\n",
      "True Beta: [0.08898067 0.60705857 0.88441801 0.6401544 ], Estimated Beta: [0.30384876 0.77322801 0.71606222 0.0207446 ]\n",
      "\n",
      "n: 100, θ: 0.3, Accuracy: 0.55\n",
      "True Beta: [0.37465786 0.3565255  0.96440219 0.14980055], Estimated Beta: [0.92370688 0.73078503 0.34191159 0.88323892]\n",
      "\n",
      "n: 100, θ: 0.5, Accuracy: 0.41\n",
      "True Beta: [0.24039413 0.40094347 0.61361472 0.12762783], Estimated Beta: [0.07727167 0.02827406 0.64145585 0.68088369]\n",
      "\n",
      "n: 1000, θ: 0.1, Accuracy: 0.56\n",
      "True Beta: [0.77435325 0.75022418 0.68664415 0.38872278], Estimated Beta: [0.14436469 0.47192996 0.63121849 0.51941654]\n",
      "\n",
      "n: 1000, θ: 0.3, Accuracy: 0.53\n",
      "True Beta: [0.08263231 0.87443636 0.09338647 0.15594453], Estimated Beta: [0.12740954 0.10668798 0.72624713 0.95884251]\n",
      "\n",
      "n: 1000, θ: 0.5, Accuracy: 0.48\n",
      "True Beta: [0.00446379 0.72002703 0.40057368 0.41914828], Estimated Beta: [0.22503169 0.88466273 0.58095721 0.57138275]\n",
      "\n",
      "n: 10000, θ: 0.1, Accuracy: 0.69\n",
      "True Beta: [0.80928504 0.23606458 0.77152841 0.81146455], Estimated Beta: [0.61706314 0.38502704 0.82621074 0.12720236]\n",
      "\n",
      "n: 10000, θ: 0.3, Accuracy: 0.61\n",
      "True Beta: [0.07189167 0.05373622 0.62655801 0.96629771], Estimated Beta: [0.39204805 0.14713333 0.62590113 0.22340902]\n",
      "\n",
      "n: 10000, θ: 0.5, Accuracy: 0.49\n",
      "True Beta: [0.35776268 0.78594618 0.25935994 0.04878031], Estimated Beta: [0.32834154 0.48816884 0.44924229 0.83946339]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_impact(n_values, theta_values, m=3):\n",
    "    k = 100\n",
    "    t = 0.01\n",
    "    lambda_ = 0.1\n",
    "    results = {}\n",
    "    \n",
    "    for n in n_values:\n",
    "        for theta in theta_values:\n",
    "            X, Y, true_B = generate_dataset(n, m, theta)\n",
    "            estimated_B, final_cost = logistic_regression(X, Y, k, t, lambda_)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = (np.dot(X, estimated_B) > 0.5).astype(int)\n",
    "            accuracy = np.mean(Y.flatten() == predictions)\n",
    "            \n",
    "            results[(n, theta)] = {\n",
    "                'accuracy': accuracy,\n",
    "                'true_B': true_B.flatten(),\n",
    "                'estimated_B': estimated_B\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "n_values = [100, 1000, 10000]\n",
    "theta_values = [0.1, 0.3, 0.5]\n",
    "\n",
    "results = evaluate_impact(n_values, theta_values)\n",
    "\n",
    "for (n, theta), result in results.items():\n",
    "    print(f\"n: {n}, θ: {theta}, Accuracy: {result['accuracy']:.2f}\")\n",
    "    print(f\"True Beta: {result['true_B']}, Estimated Beta: {result['estimated_B']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dfbe53-c26d-481b-988b-546438e3bff6",
   "metadata": {},
   "source": [
    "### Observations\n",
    "As we can see that \n",
    "\n",
    "- accuracy is directly proportional to n\n",
    "- accuracy is indirectly proportional to theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24b7a7c-b52d-446b-a84a-eff560713d49",
   "metadata": {},
   "source": [
    "### Derivation of the partial derivative of the cost function with respect to the parameters of the model.\n",
    "\n",
    "The partial derivative of the logistic regression cost function (negative log-likelihood) with respect to the parameters ( \\beta ) is given by:\n",
    "\n",
    "$$nabla L(\\beta) = X^T \\cdot (h_\\beta(X) - Y) $$\n",
    "\n",
    "Where:\n",
    "\n",
    "    ( h_\\beta(X) = \\frac{1}{1 + \\exp(-X \\cdot \\beta)} ), the logistic function.\n",
    "    ( X ) is the matrix of features.\n",
    "    ( Y ) is the vector of observed labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617f3ea-e4c7-4a81-a4b3-e279bba70ac9",
   "metadata": {},
   "source": [
    "Add L1 and L2 regularization to the Logistic Regression cost function. How does this impact the models\n",
    "learnt? How does the choice of regularization constant impact the β vector learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d74c3747-5509-4c57-8b0c-8caf04c15b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming `generate_dataset` is already defined\n",
    "\n",
    "def logistic_regression(X, Y, k, tau, lambda_, regularization=None, alpha=0.1):\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "        \n",
    "    def cost_function(X, Y, B):\n",
    "        n = len(Y)\n",
    "        predictions = sigmoid(np.dot(X, B))\n",
    "        term1 = np.dot(Y.T, np.log(predictions))\n",
    "        term2 = np.dot((1 - Y).T, np.log(1 - predictions))\n",
    "        base_cost = -1/n * (term1 + term2)\n",
    "        \n",
    "        if regularization == 'L1':\n",
    "            reg_cost = alpha * np.sum(np.abs(B))\n",
    "        elif regularization == 'L2':\n",
    "            reg_cost = alpha / 2 * np.sum(B**2)\n",
    "        else:\n",
    "            reg_cost = 0\n",
    "\n",
    "        return base_cost + reg_cost\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    B = np.random.random(size=(X.shape[1], 1))\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    cost = cost_function(X, Y, B)\n",
    "    \n",
    "    for _ in range(k):\n",
    "        prev_cost = cost\n",
    "\n",
    "        predictions = sigmoid(np.dot(X, B))\n",
    "        gradient = np.dot(X.T, (predictions - Y)) / n\n",
    "        \n",
    "        if regularization == 'L1':\n",
    "            reg_gradient = alpha * np.sign(B)\n",
    "        elif regularization == 'L2':\n",
    "            reg_gradient = alpha * B\n",
    "        else:\n",
    "            reg_gradient = 0\n",
    "        \n",
    "        B = B - lambda_ * (gradient + reg_gradient)\n",
    "        \n",
    "        cost = cost_function(X, Y, B)\n",
    "\n",
    "        if abs(cost - prev_cost) < tau:\n",
    "            break\n",
    "\n",
    "    return B.flatten(), cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c543d9bf-1cde-4ce8-8953-f8da94da0578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization: L1, Alpha: 0.01, Accuracy: 0.46, Final Cost: 0.68\n",
      "Estimated Beta: [0.05465978 0.09095697 0.02414507 0.48102008]\n",
      "\n",
      "Regularization: L1, Alpha: 0.1, Accuracy: 0.61, Final Cost: 0.88\n",
      "Estimated Beta: [0.63430422 0.45244907 0.9619378  0.13119283]\n",
      "\n",
      "Regularization: L1, Alpha: 1, Accuracy: 0.39, Final Cost: 0.88\n",
      "Estimated Beta: [-0.04450389 -0.04049101 -0.0212851  -0.06338848]\n",
      "\n",
      "Regularization: L2, Alpha: 0.01, Accuracy: 0.66, Final Cost: 0.65\n",
      "Estimated Beta: [0.77195483 0.28874657 0.7265586  0.56620302]\n",
      "\n",
      "Regularization: L2, Alpha: 0.1, Accuracy: 0.60, Final Cost: 0.69\n",
      "Estimated Beta: [0.50474296 0.19653331 0.72598423 0.31086642]\n",
      "\n",
      "Regularization: L2, Alpha: 1, Accuracy: 0.45, Final Cost: 0.71\n",
      "Estimated Beta: [0.26125754 0.06358087 0.08856663 0.20331655]\n",
      "\n",
      "Regularization: None, Alpha: 0.01, Accuracy: 0.63, Final Cost: 0.63\n",
      "Estimated Beta: [0.72974585 0.60273043 0.18567731 0.37445455]\n",
      "\n",
      "Regularization: None, Alpha: 0.1, Accuracy: 0.60, Final Cost: 0.65\n",
      "Estimated Beta: [0.41163251 0.60572946 0.60382956 0.8553233 ]\n",
      "\n",
      "Regularization: None, Alpha: 1, Accuracy: 0.59, Final Cost: 0.65\n",
      "Estimated Beta: [0.3309111  0.67737891 0.77271872 0.62570915]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_regularization(n=1000, m=3, theta=0.3, k=100, tau=0.01, lambda_=0.1):\n",
    "    alpha_values=[0.01, 0.1, 1]\n",
    "    regularization_methods = ['L1', 'L2', None]\n",
    "    results = {}\n",
    "\n",
    "    # Generate Data\n",
    "    X, Y, true_B = generate_dataset(n, m, theta)\n",
    "\n",
    "    for reg in regularization_methods:\n",
    "        for alpha in alpha_values:\n",
    "            estimated_B, final_cost = logistic_regression(X, Y, k, tau, lambda_, regularization=reg, alpha=alpha)\n",
    "            predictions = (np.dot(X, estimated_B) > 0.5).astype(int)\n",
    "            accuracy = np.mean(Y.flatten() == predictions)\n",
    "            \n",
    "            results[(reg, alpha)] = {\n",
    "                'accuracy': accuracy,\n",
    "                'estimated_Beta': estimated_B,\n",
    "                'final_cost': final_cost.item()  # Ensure final_cost is a scalar\n",
    "            }\n",
    "\n",
    "    return results\n",
    "\n",
    "results = evaluate_regularization()\n",
    "\n",
    "for (reg, alpha), result in results.items():\n",
    "    print(f\"Regularization: {reg}, Alpha: {alpha}, Accuracy: {result['accuracy']:.2f}, Final Cost: {result['final_cost']:.2f}\")\n",
    "    print(f\"Estimated Beta: {result['estimated_Beta']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb71939-7c0a-427a-b9d5-a06b09ece55a",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "As We can see after comparing with No Regularization:\n",
    "\n",
    "For L1 Regularization:\n",
    "- The Model is showing lower accuracy in this case.\n",
    "\n",
    "For L2 Regularization:\n",
    "- The Model is showing higher accuracy than No Regularization But Only when `Alpha = 0.01`\n",
    "- That is Alpha should be very low for making a logistic regression model of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "116184fa-e3c8-4af6-84c8-b95613f7b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRegression:\n",
    "    \"\"\"\n",
    "    A base class for regression models.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        raise NotImplementedError(\"Subclass must implement this method.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError(\"Subclass must implement this method.\")\n",
    "\n",
    "\n",
    "class LinearRegression(BaseRegression):\n",
    "    \"\"\"\n",
    "    Linear Regression model class.\n",
    "    \"\"\"\n",
    "    def fit(self, X, Y):\n",
    "        n, m = X.shape\n",
    "        self.weights = np.zeros(m)\n",
    "        for _ in range(self.num_iterations):\n",
    "            predictions = np.dot(X, self.weights)\n",
    "            error = predictions - Y\n",
    "            gradient = np.dot(X.T, error) / n\n",
    "            self.weights -= self.learning_rate * gradient\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights)\n",
    "\n",
    "\n",
    "class LogisticRegression(BaseRegression):\n",
    "    \"\"\"\n",
    "    Logistic Regression model class.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, threshold=0.5):\n",
    "        super().__init__(learning_rate, num_iterations)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        n, m = X.shape\n",
    "        self.weights = np.zeros(m)\n",
    "        for _ in range(self.num_iterations):\n",
    "            predictions = self.sigmoid(np.dot(X, self.weights))\n",
    "            error = predictions - Y\n",
    "            gradient = np.dot(X.T, error) / n\n",
    "            self.weights -= self.learning_rate * gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.sigmoid(np.dot(X, self.weights)) >= self.threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef65b96-5b0b-4826-a2e1-762ea4031600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
